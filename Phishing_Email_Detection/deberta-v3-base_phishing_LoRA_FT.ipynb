{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80xvsmaVlrwx"
   },
   "source": [
    "#Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.16.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.16.5-py3-none-any.whl (247 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.4/247.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pyarrow, colorlog, pandas, alembic, optuna, evaluate\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
      "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed alembic-1.16.5 colorlog-6.9.0 evaluate-0.4.5 optuna-4.5.0 pandas-2.3.2 pyarrow-21.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets transformers accelerate pandas pyarrow chardet nltk spacy evaluate optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 transformers==4.44.2 evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tN317b2-lwen"
   },
   "source": [
    "#Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re,string\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torch.utils.data import DataLoader\n",
    "import nltk\n",
    "import optuna\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print('Torch:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yx3tbwTkly3P"
   },
   "source": [
    "#Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip = zipfile.ZipFile(\"archive.zip\", 'r')\n",
    "zip.extractall(\"/content/\")\n",
    "zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'Email Text', 'Email Type'],\n",
      "    num_rows: 18650\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#From huggingface\n",
    "# dataset = load_dataset('imdb')\n",
    "# print(dataset)\n",
    "\n",
    "#Custom\n",
    "dataset = load_dataset(\"csv\",data_files={'data':'Phishing_Email.csv'})['data']\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'Email Text', 'Email Type'],\n",
      "    num_rows: 15000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(seed=42).select(range(15000))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 697, 'Email Text': 'URL: http://boingboing.net/#85490404\\nDate: Not suppliedA new KPMG study concludes that the RIAA and its member companies are hurting \\nthemselves by focusing on cracking down on P2P sharing instead of figuring out \\nways to earn a living with it.     Media companies must put less emphasis on protecting digital content and \\n    instead find ways to make money from digital music and movies if they hope \\n    to beat back copyright pirates who threaten their businesses, according to \\n    a study released on Wednesday from KPMG...     \"They complain about the Napsters,\" she said, referring to the bankrupt \\n    music swap site that was found to violate U.S. copyright laws. \"But why do \\n    the Napsters exist, because the marketplace wants them.\"     Steel said that if the issue \"is not on boardroom table ... then that \\n    boardroom has problems.\"  Link[1] Discuss[2] (_Thanks, Michael!_)[1] http://story.news.yahoo.com/news?tmpl=story&ncid=581&e=3&cid=581&u=/nm/20020925/tc_nm/media_kpmg_dc\\n[2] http://www.quicktopic.com/boing/H/cyfuTEBbTuVRG\\n', 'Email Type': 'Safe Email'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'Email Text', 'Email Type'],\n",
      "        num_rows: 12000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'Email Text', 'Email Type'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(seed=42, test_size=0.2)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0g4lx6M5q0gb"
   },
   "source": [
    "#Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop nulls and fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_url=True\n",
    "re_html=True\n",
    "re_stopwords=False\n",
    "lower=True\n",
    "re_punc=False\n",
    "\n",
    "urls = re.compile(r\"(www\\.\\S+|https?://\\S+)\")\n",
    "htmls = re.compile(r\"(<.*?>)\")\n",
    "stopwords = set(stopwords.words(\"english\")) if re_stopwords else set()\n",
    "punct = str.maketrans(\"\",\"\",string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "  if re_url:\n",
    "    x=urls.sub(\" \",x)\n",
    "  if re_html:\n",
    "    x=htmls.sub(\" \",x)\n",
    "  if lower:\n",
    "    x=x.lower()\n",
    "  if re_stopwords:\n",
    "    x= \" \".join([w for w in x.split() if w not in stopwords])\n",
    "  if re_punc:\n",
    "    x=x.translate(punct)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clean(batch):\n",
    "  return {\"Email Text\":[clean_text(x if x is not None else \"\") for x in batch[\"Email Text\"]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ac626823ee4b8baef4b07f647bcd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781e7f8e02774f7f83bfc5b55a50ea2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 7132, 'Email Text': 're : phara . macy colo ured 27 hello , vislt our great pharmac - ybymail shop and save 75 % v rava luma lenci is , man lag l mb al yother . have a good day . p . s . you will be pieasantiy surprised with our prlcess ; - ) courage . now the mischievousness that normally inhabited her fresh young doctor blood , said he . he was a short , broad man of five - and - fo heavily upon a stout ebony cane . after him , in the uniform of a who the hell may you be ? he exploded . colour and grow troubled once more . could i be guilty of that ? protested the captain . i realize t that restless spirit by which he was imbued . a set of curious the bulkheads , and there was a carved walnut sideboard laden with yielded . already bishop was moving down the line . for mr . blood we might send don diego de espinosa in a boat manned by his though it took a fortnight , blood bubbled him . he sent me and mo stately red - hulled frigate , flying the english ensign . fortunes of the duke of monmouth . he was a sturdy , resolute fell fraught with loss of dignity . but there were those volunteers th', 'Email Type': 'Phishing Email'}\n"
     ]
    }
   ],
   "source": [
    "dataset=dataset.map(apply_clean,batched=True)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ry4v5ey-tbiJ"
   },
   "source": [
    "#Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bccf49c7d114fb286b1484e35274c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'microsoft/deberta-v3-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d3b2589f3e435c832dbacd0c3c2385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7695df46ab2e4edfaa9d1d6221df89ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d16c140bfad41cc957c82a4ea0a804d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model_name,use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_len = 256/512 or wahtever , we used 256+128 here\n",
    "max_len = 512\n",
    "\n",
    "def tokenize(batch):\n",
    "  return tokenizer(batch['Email Text'], truncation=True, max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'Email Text', 'Email Type'],\n",
      "        num_rows: 12000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'Email Text', 'Email Type'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0070e37474ab4f8b9fd9b9945372cbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8bec4f0cdc4982bb238420a69526f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Email Type': ['Phishing Email', 'Phishing Email', 'Phishing Email'], 'input_ids': [[1, 961, 877, 845, 18521, 323, 12412, 608, 82769, 15140, 569, 1824, 12018, 366, 21161, 19011, 316, 426, 30893, 1207, 341, 2982, 2319, 3410, 1638, 263, 1213, 3588, 4639, 1942, 507, 55922, 2531, 14392, 5352, 53535, 269, 366, 642, 15426, 2531, 31806, 1564, 2982, 10705, 323, 286, 266, 397, 406, 323, 845, 323, 1550, 323, 274, 296, 282, 7085, 54978, 90075, 3193, 275, 316, 845, 20226, 34939, 2600, 341, 1263, 7253, 323, 394, 262, 37474, 2021, 272, 3406, 22726, 342, 1576, 856, 2278, 1452, 366, 357, 313, 323, 313, 284, 266, 782, 366, 3658, 642, 265, 773, 341, 263, 341, 18835, 4605, 1064, 266, 29136, 50162, 295, 473, 323, 385, 417, 366, 267, 262, 6945, 265, 266, 328, 262, 4210, 372, 274, 282, 1102, 313, 15974, 323, 2909, 263, 1618, 12331, 704, 310, 323, 387, 584, 282, 4374, 265, 272, 1102, 26084, 262, 6911, 323, 584, 2544, 1941, 272, 24513, 2816, 293, 319, 313, 284, 52716, 323, 266, 487, 265, 5348, 262, 51621, 268, 366, 263, 343, 284, 266, 11273, 20643, 50064, 31348, 275, 20120, 323, 637, 18327, 284, 1458, 444, 262, 683, 323, 270, 25242, 323, 1452, 301, 520, 1339, 418, 65944, 718, 24735, 547, 14485, 267, 266, 2750, 30010, 293, 315, 651, 278, 681, 266, 32179, 366, 1452, 91253, 417, 323, 313, 1253, 351, 263, 11269, 40011, 1267, 341, 17544, 569, 49300, 366, 4021, 262, 13247, 104122, 323, 24088, 265, 262, 32319, 265, 18528, 15052, 323, 313, 284, 266, 10111, 366, 50416, 2599, 32380, 275, 1265, 265, 10881, 323, 304, 343, 332, 421, 4417, 6554, 2], [1, 5595, 955, 532, 11355, 338, 274, 282, 526, 264, 5595, 955, 337, 6674, 277, 262, 2011, 1102, 1102, 1102, 12018, 341, 370, 355, 387, 2030, 280, 297, 1330, 272, 900, 275, 356, 2613, 323, 275, 561, 268, 2898, 1039, 263, 1877, 3124, 735, 366, 278, 382, 268, 266, 900, 272, 837, 634, 264, 1024, 1147, 323, 2898, 4644, 292, 262, 18234, 265, 3201, 565, 272, 877, 341, 977, 4639, 265, 5791, 5857, 3080, 288, 289, 941, 262, 3890, 382, 268, 425, 341, 936, 4639, 288, 563, 263, 621, 4639, 288, 1613, 16423, 287, 2507, 1263, 341, 266, 1220, 269, 19557, 469, 392, 768, 267, 25179, 341, 376, 321, 265, 469, 453, 62436, 1046, 470, 267, 266, 613, 537, 289, 266, 2692, 3517, 323, 341, 1600, 4639, 1411, 411, 1190, 275, 266, 9263, 1506, 288, 262, 326, 306, 332, 47536, 323, 341, 1259, 4639, 1411, 306, 332, 288, 374, 289, 4270, 264, 289, 292, 374, 335, 262, 2898, 3651, 323, 283, 274, 295, 398, 366, 266, 5791, 2898, 295, 1433, 264, 1012, 288, 356, 326, 3948, 265, 399, 274, 685, 289, 374, 323, 378, 5857, 11358, 265, 5292, 366, 14559, 366, 99978, 366, 18712, 263, 4028, 323, 636, 277, 378, 4644, 366, 278, 382, 268, 823, 4851, 272, 274, 296, 282, 262, 3890, 265, 266, 5791, 2898, 288, 347, 582, 267, 290, 432, 323, 296, 274, 8078, 1102, 1102, 1102, 1102, 584, 382, 10891, 331, 4367, 262, 11755, 3683, 270, 9334, 388, 263, 286, 331, 299, 7209, 270, 16873, 265, 349, 323, 584, 382, 10891, 331, 8270, 618, 264, 286, 4345, 494, 347, 265, 262, 410, 11341, 267, 262, 658, 323, 584, 1049, 286, 266, 937, 4931, 267, 26948, 8294, 38245, 263, 286, 4345, 93030, 4219, 13302, 333, 366, 48681, 18448, 263, 584, 481, 1049, 4367, 59517, 1165, 15081, 263, 269, 10657, 3841, 27546, 323, 312, 750, 366, 307, 266, 1177, 264, 4046, 307, 366, 296, 553, 274, 361, 264, 5595, 955, 532, 262, 1720, 370, 1019, 1361, 265, 3096, 277, 262, 2011, 323, 584, 296, 553, 274, 934, 2774, 2244, 272, 1012, 292, 3607, 1906, 264, 25435, 295, 799, 263, 380, 264, 5595, 1147, 323, 378, 2244, 281, 1538, 263, 639, 264, 799, 263, 295, 282, 262, 1478, 457, 432, 289, 1142, 337, 274, 281, 6674, 277, 262, 2011, 323, 312, 750, 1743, 639, 264, 796, 1169, 270, 3358, 572, 323, 291, 750, 269, 266, 516, 270, 356, 1575, 1234, 287, 2844, 289, 2416, 1263, 323, 262, 2244, 267, 291, 750, 281, 298, 364, 397, 270, 934, 2774, 304, 270, 792, 2613, 335, 321, 267, 613, 323, 2127, 3063, 263, 1686, 905, 2614, 296, 2088, 262, 370, 1591, 292, 291, 750, 323, 278, 296, 327, 527, 274, 2136, 10709, 277, 361, 264, 1886, 4507, 263, 36111, 324, 306, 722, 280, 297, 350, 854, 366, 361, 264, 1662, 562, 334, 266, 3890, 366, 263, 361, 264, 380, 1019, 3296, 283, 3681, 264, 31202, 442, 402, 262, 370, 5638, 16382, 323, 275, 324, 386, 934, 2774, 2340, 277, 262, 594, 366, 278, 269, 314, 326, 272, 343, 284, 311, 412, 270, 355, 2], [1, 353, 8532, 286, 20120, 459, 117751, 1540, 1698, 6696, 3090, 2673, 667, 1540, 1698, 815, 2848, 263, 8005, 877, 5990, 25372, 268, 36395, 287, 610, 673, 1263, 15771, 21317, 265, 86245, 1540, 1540, 267, 948, 2049, 267, 1698, 341, 2241, 4844, 323, 262, 483, 382, 1550, 1031, 14858, 5210, 265, 32976, 10672, 843, 366, 14261, 19894, 275, 266, 3199, 5076, 275, 14364, 9554, 74111, 40237, 287, 266, 19356, 667, 49806, 341, 3787, 483, 1263, 303, 1592, 301, 1540, 1540, 1117, 293, 322, 264, 525, 50384, 4639, 323, 610, 673, 1293, 1980, 341, 437, 1296, 877, 767, 323, 9632, 752, 2481, 877, 15424, 50384, 1578, 10618, 877, 341, 2673, 667, 1540, 1428, 281, 7952, 419, 960, 266, 7475, 341, 8847, 14887, 19415, 1080, 1624, 18247, 1540, 1540, 959, 264, 2389, 341, 610, 673, 1293, 2240, 264, 3440, 264, 353, 5068, 542, 459, 268, 341, 610, 673, 1293, 269, 8448, 270, 1074, 775, 264, 1031, 1080, 263, 1698, 266, 55625, 268, 16436, 341, 610, 673, 4693, 850, 49431, 277, 262, 3090, 1080, 1698, 8706, 287, 594, 14095, 419, 466, 36912, 667, 4593, 1263, 341, 3090, 2673, 667, 1540, 1698, 2190, 531, 3470, 1062, 264, 27820, 63583, 13038, 341, 610, 673, 269, 8258, 2638, 264, 676, 752, 8545, 30797, 63583, 13038, 341, 483, 268, 1074, 1808, 269, 3787, 293, 10386, 87861, 70635, 3342, 366, 101312, 265, 5990, 25372, 268, 366, 652, 877, 9914, 267, 405, 266, 2241, 537, 265, 1698, 6696, 3329, 501, 12267, 270, 5990, 25372, 268, 323, 262, 483, 382, 1550, 353, 17962, 275, 32976, 10672, 843, 1376, 51875, 27820, 1318, 782, 263, 455, 341, 1384, 772, 1924, 1270, 341, 341, 47808, 341, 341, 287, 460, 4016, 1263, 341, 341, 9409, 453, 366, 2589, 5990, 25372, 268, 25875, 323, 561, 1577, 272, 32976, 10672, 843, 25875, 323, 366, 262, 483, 382, 1550, 3199, 5076, 1940, 366, 5726, 272, 9914, 2049, 288, 262, 40801, 452, 525, 341, 989, 341, 27484, 301, 5486, 286, 16396, 323, 262, 40801, 452, 525, 341, 989, 341, 27484, 301, 1540, 1540, 269, 262, 362, 265, 299, 267, 61602, 1540, 375, 287, 392, 1263, 371, 6383, 9914, 1309, 7325, 262, 1698, 9970, 9554, 667, 53908, 19487, 268, 2319, 4844, 7828, 288, 266, 3613, 265, 2407, 392, 366, 466, 2673, 1526, 323, 1031, 14816, 267, 262, 19487, 268, 2319, 4844, 286, 74769, 54435, 569, 459, 2165, 9590, 1008, 1698, 288, 1624, 265, 376, 323, 392, 264, 453, 323, 392, 7840, 99616, 12655, 1526, 287, 4443, 18887, 407, 1263, 605, 406, 323, 65698, 452, 1540, 29485, 1540, 6696, 372, 282, 322, 264, 9667, 7147, 1540, 1540, 14997, 12655, 1526, 287, 466, 2165, 18887, 1263, 323, 32976, 10672, 25875, 323, 1049, 3807, 30789, 360, 453, 366, 97381, 6072, 265, 50402, 2226, 4076, 263, 117751, 1540, 1698, 1540, 3468, 268, 546, 262, 43419, 265, 291, 5647, 117751, 1540, 1698, 612, 323, 266, 13295, 25590, 9913, 1540, 2932, 269, 850, 277, 262, 567, 9554, 667, 53908, 19487, 268, 2319, 7032, 323, 1423, 265, 291, 514, 37044, 1540, 2263, 262, 483, 267, 6428, 7636, 9914, 4786, 277, 378, 7208, 323, 483, 2636, 5863, 2226, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"Email Text\",\"Unnamed: 0\"])\n",
    "print(tokenized[\"train\"][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad75ad6662f4e8cb47a63764ea566a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a54bf239a543c7b6c3e10319fe2fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 961, 877, 845, 18521, 323, 12412, 608, 82769, 15140, 569, 1824, 12018, 366, 21161, 19011, 316, 426, 30893, 1207, 341, 2982, 2319, 3410, 1638, 263, 1213, 3588, 4639, 1942, 507, 55922, 2531, 14392, 5352, 53535, 269, 366, 642, 15426, 2531, 31806, 1564, 2982, 10705, 323, 286, 266, 397, 406, 323, 845, 323, 1550, 323, 274, 296, 282, 7085, 54978, 90075, 3193, 275, 316, 845, 20226, 34939, 2600, 341, 1263, 7253, 323, 394, 262, 37474, 2021, 272, 3406, 22726, 342, 1576, 856, 2278, 1452, 366, 357, 313, 323, 313, 284, 266, 782, 366, 3658, 642, 265, 773, 341, 263, 341, 18835, 4605, 1064, 266, 29136, 50162, 295, 473, 323, 385, 417, 366, 267, 262, 6945, 265, 266, 328, 262, 4210, 372, 274, 282, 1102, 313, 15974, 323, 2909, 263, 1618, 12331, 704, 310, 323, 387, 584, 282, 4374, 265, 272, 1102, 26084, 262, 6911, 323, 584, 2544, 1941, 272, 24513, 2816, 293, 319, 313, 284, 52716, 323, 266, 487, 265, 5348, 262, 51621, 268, 366, 263, 343, 284, 266, 11273, 20643, 50064, 31348, 275, 20120, 323, 637, 18327, 284, 1458, 444, 262, 683, 323, 270, 25242, 323, 1452, 301, 520, 1339, 418, 65944, 718, 24735, 547, 14485, 267, 266, 2750, 30010, 293, 315, 651, 278, 681, 266, 32179, 366, 1452, 91253, 417, 323, 313, 1253, 351, 263, 11269, 40011, 1267, 341, 17544, 569, 49300, 366, 4021, 262, 13247, 104122, 323, 24088, 265, 262, 32319, 265, 18528, 15052, 323, 313, 284, 266, 10111, 366, 50416, 2599, 32380, 275, 1265, 265, 10881, 323, 304, 343, 332, 421, 4417, 6554, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {\"Safe Email\": 0, \"Phishing Email\": 1}\n",
    "\n",
    "def map_labels_to_int(batch):\n",
    "    return {\"labels\": [label_mapping[label] for label in batch[\"Email Type\"]]}\n",
    "\n",
    "# Apply the mapping to the dataset\n",
    "tokenized = tokenized.map(map_labels_to_int, batched=True)\n",
    "\n",
    "# Remove the original 'Email Type' column\n",
    "tokenized = tokenized.remove_columns([\"Email Type\"])\n",
    "\n",
    "print(tokenized[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 12000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Counter({0: 7328, 1: 4672})\n",
      "Test: Counter({0: 1800, 1: 1200})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# if your column is called \"label\"\n",
    "train_labels = tokenized[\"train\"][\"labels\"]\n",
    "test_labels = tokenized[\"test\"][\"labels\"]\n",
    "\n",
    "print(\"Train:\", Counter(train_labels))\n",
    "print(\"Test:\", Counter(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5zZZv7DvdSG"
   },
   "source": [
    "#Padding & Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(tokenized[\"train\"], batch_size=8, collate_fn=data_collator, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 509])\n",
      "token_type_ids torch.Size([8, 509])\n",
      "attention_mask torch.Size([8, 509])\n",
      "labels torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bc6fd74fe24cc3b105b0b1f050571c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63ef27298904127a607669bd9ef5ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready & saved!\n"
     ]
    }
   ],
   "source": [
    "tokenized.save_to_disk(\"processed_phishing_dataset_DeBERTa-V3-base\")\n",
    "print(\"Dataset ready & saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: processed_phishing_dataset_DeBERTa-V3-base/ (stored 0%)\n",
      "  adding: processed_phishing_dataset_DeBERTa-V3-base/train/ (stored 0%)\n",
      "  adding: processed_phishing_dataset_DeBERTa-V3-base/train/state.json (deflated 39%)\n",
      "  adding: processed_phishing_dataset_DeBERTa-V3-base/train/data-00000-of-00001.arrow (deflated 74%)\n",
      "  adding: processed_phishing_dataset_DeBERTa-V3-base/train/dataset_info.json (deflated 66%)\n",
      "  adding: processed_phishing_dataset_DeBERTa-V3-base/test/ (stored 0%)\n",
      "  adding: processed_phishing_dataset_DeBERTa-V3-base/test/state.json (deflated 39%)\n",
      "  adding: processed_phishing_dataset_DeBERTa-V3-base/test/data-00000-of-00001.arrow (deflated 74%)\n",
      "  adding: processed_phishing_dataset_DeBERTa-V3-base/test/dataset_info.json (deflated 66%)\n",
      "  adding: processed_phishing_dataset_DeBERTa-V3-base/dataset_dict.json (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r processed_phishing_dataset_DeBERTa-V3-base.zip ./processed_phishing_dataset_DeBERTa-V3-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pFr0hPIvJYa"
   },
   "source": [
    "#Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134d2df8b6e44ecab74e0feb6a7de1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_loaded = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Px4VUWTYvM5B"
   },
   "source": [
    "#Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a835846b57034c3591039493b436d1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e226d46fe86241bdbfab0e069c3cfef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53454425b9e434b8fd68f6b92bee992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": acc_metric.compute(predictions=preds, references=labels),\n",
    "        \"f1\": f1_metric.compute(predictions=preds, references=labels, average=\"weighted\"),\n",
    "        \"recall\": recall_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCZ9GiUQkzXd"
   },
   "source": [
    "#LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DebertaV2Model(\n",
      "  (embeddings): DebertaV2Embeddings(\n",
      "    (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): DebertaV2Encoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x DebertaV2Layer(\n",
      "        (attention): DebertaV2Attention(\n",
      "          (self): DisentangledSelfAttention(\n",
      "            (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (pos_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): DebertaV2SelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): DebertaV2Intermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): DebertaV2Output(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rel_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_loaded.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['query_proj','key_proj','value_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model_loaded, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 886,274 || all params: 185,309,956 || trainable%: 0.4783\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keoyR7_5vO3K"
   },
   "source": [
    "#Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2919566463.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deberta-classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfJ0bocyvXpr"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makhyar\u001b[0m (\u001b[33makhyar-university-of-engineering-and-technology-lahore\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250828_080317-zh91haz5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/akhyar-university-of-engineering-and-technology-lahore/huggingface/runs/zh91haz5' target=\"_blank\">feasible-butterfly-15</a></strong> to <a href='https://wandb.ai/akhyar-university-of-engineering-and-technology-lahore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/akhyar-university-of-engineering-and-technology-lahore/huggingface' target=\"_blank\">https://wandb.ai/akhyar-university-of-engineering-and-technology-lahore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/akhyar-university-of-engineering-and-technology-lahore/huggingface/runs/zh91haz5' target=\"_blank\">https://wandb.ai/akhyar-university-of-engineering-and-technology-lahore/huggingface/runs/zh91haz5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 340/1125 09:06 < 21:08, 0.62 it/s, Epoch 0.90/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 33:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.072302</td>\n",
       "      <td>{'accuracy': 0.9703333333333334}</td>\n",
       "      <td>{'f1': 0.9704321825452422}</td>\n",
       "      <td>{'recall': 0.9703333333333334}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.062556</td>\n",
       "      <td>{'accuracy': 0.9746666666666667}</td>\n",
       "      <td>{'f1': 0.9747319948425125}</td>\n",
       "      <td>{'recall': 0.9746666666666667}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.063105</td>\n",
       "      <td>{'accuracy': 0.974}</td>\n",
       "      <td>{'f1': 0.9740544607033279}</td>\n",
       "      <td>{'recall': 0.974}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.9703333333333334}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.9704321825452422}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.9703333333333334}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.9746666666666667}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.9747319948425125}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.9746666666666667}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.974}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.9740544607033279}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.974}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1125, training_loss=0.10089683956570096, metrics={'train_runtime': 2040.7501, 'train_samples_per_second': 17.641, 'train_steps_per_second': 0.551, 'total_flos': 9267725028142080.0, 'train_loss': 0.10089683956570096, 'epoch': 3.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGR-duwdHs_o"
   },
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.9746666666666667}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.9747319948425125}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.9746666666666667}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBERTA metrics: {'eval_loss': 0.0625561997294426, 'eval_accuracy': {'accuracy': 0.9746666666666667}, 'eval_f1': {'f1': 0.9747319948425125}, 'eval_recall': {'recall': 0.9746666666666667}, 'eval_runtime': 71.5237, 'eval_samples_per_second': 41.944, 'eval_steps_per_second': 5.243, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"DEBERTA metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [\"deberta-v3-base\"]\n",
    "\n",
    "accuracy = metrics[\"eval_accuracy\"][\"accuracy\"]\n",
    "f1 = metrics[\"eval_f1\"][\"f1\"]\n",
    "x = range(len(models))\n",
    "width = 0.2\n",
    "\n",
    "plt.bar([i - width/2 for i in x], accuracy, width=width, label=\"Accuracy\")\n",
    "plt.bar([i + width/2 for i in x], f1, width=width, label=\"F1 Score\")\n",
    "\n",
    "plt.xticks(x, models)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"LoRA FT Results\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjIZeysZoEFf"
   },
   "source": [
    "#Saving Model + config + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./deberta-v3-classifier/tokenizer_config.json',\n",
       " './deberta-v3-classifier/special_tokens_map.json',\n",
       " './deberta-v3-classifier/spm.model',\n",
       " './deberta-v3-classifier/added_tokens.json',\n",
       " './deberta-v3-classifier/tokenizer.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./deberta-v3-classifier\")  # saves model + config\n",
    "tokenizer.save_pretrained(\"./deberta-v3-classifier\")  # saves tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96mkUbmnohuz"
   },
   "source": [
    "#Zip & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: deberta-v3-classifier/ (stored 0%)\n",
      "  adding: deberta-v3-classifier/README.md (deflated 66%)\n",
      "  adding: deberta-v3-classifier/training_args.bin (deflated 53%)\n",
      "  adding: deberta-v3-classifier/spm.model (deflated 50%)\n",
      "  adding: deberta-v3-classifier/adapter_model.safetensors (deflated 7%)\n",
      "  adding: deberta-v3-classifier/tokenizer.json (deflated 77%)\n",
      "  adding: deberta-v3-classifier/tokenizer_config.json (deflated 73%)\n",
      "  adding: deberta-v3-classifier/added_tokens.json (stored 0%)\n",
      "  adding: deberta-v3-classifier/special_tokens_map.json (deflated 50%)\n",
      "  adding: deberta-v3-classifier/adapter_config.json (deflated 56%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r deberta-v3-classifier.zip ./deberta-v3-classifier"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
