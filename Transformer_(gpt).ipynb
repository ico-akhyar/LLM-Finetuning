{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjimNV-d0SNi"
   },
   "source": [
    "#Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1754630987953,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "amtnnXJovG35"
   },
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ohSPIA4z0r1"
   },
   "source": [
    "#Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4167,
     "status": "ok",
     "timestamp": 1754630993316,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "I72I5TkAS00o"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yatVHilO6nH5"
   },
   "source": [
    "#Shift On GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHXLUBjkc0YD"
   },
   "source": [
    "This will make the training use GPU instead of CPU. Using GPU will make training and model usage much faster ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1754630994540,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "a0VfVuSv6qYJ",
    "outputId": "f46ab9e5-b6c8-4382-f87a-7a9e16d2a310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot1KZgzPzxon"
   },
   "source": [
    "#Tokenize + Encode Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUf5cLsYdIxh"
   },
   "source": [
    "This code will allocate a specific number code to each charater of given text called tokens.\n",
    "we make 2 dictionaries as given below:\n",
    "1. stoi (key: character, value: integer_id)\n",
    "2. itos (key: integer_id, value: character)\n",
    "\n",
    "encode function converts the text into tokens (integer_ids) while decode function convert the encoded tokens again into original text according to the saved dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1754630995974,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "bmklDvSHQ9Af"
   },
   "outputs": [],
   "source": [
    "txt=\"\"\"The sun was setting behind the hills. Birds flew across the orange sky, heading home. A boy sat on the grass, watching the clouds move slowly. He smiled, feeling the cool wind on his face. It was a peaceful evening, and everything felt calm. He didn’t want the moment to end.\\n\"\"\" * 100\n",
    "\n",
    "chars = sorted(list(set(txt)))\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "\n",
    "def encode(s):\n",
    "  return [stoi[c] for c in s]\n",
    "def decode(l):\n",
    "  return ''.join([itos[n] for n in l])\n",
    "\n",
    "\n",
    "x = torch.tensor(encode(txt)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeEw0cXwzq8x"
   },
   "source": [
    "#Make Tiny Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVCfoiq-KitM"
   },
   "source": [
    "This code is used to make many different data batches from a single dataset.\n",
    "Like previously in ML the dataset is like (x,y) where x is the input and y is the output (prediction), samelike that x is the input text and y is the output text. Because the transformer will predict what user wants to write next so we make dataset from the original data like some words (block size) as input (x) and some next words (block size) as output (y).\n",
    "\n",
    "\n",
    "First we make two lists x & y. then we loop over the range of len(seq) - block_size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWOnRwYDNrPG"
   },
   "source": [
    "For Example:\n",
    "seq = [2,3,1,6,5,9]\n",
    "block_size = 3\n",
    "\n",
    "so, len(seq) = 6\n",
    "and (len(seq) - block_size) = 6-3 =3\n",
    "\n",
    "i will iterate through 0->3\n",
    "for i = 0:\n",
    "x.append(2:0+3)\n",
    "-> x.append(2:3)\n",
    "-> x becomes [2,3,1]\n",
    "and y becomes [3,1,6]\n",
    "in this way we get many batches (len(seq) - block_size)\n",
    "\n",
    "At the end it becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tky00XuNyV9"
   },
   "source": [
    "x = [[2,3,1], [3,1,6], [1,6,5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuK0I2qLN8Hv"
   },
   "source": [
    "\n",
    "y = [[3,1,6], [1,6,5], [6,5,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1754630998110,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "QEYlA-dzZORw"
   },
   "outputs": [],
   "source": [
    "def get_batch(seq, block_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(seq) - block_size):\n",
    "        x.append(seq[i:i + block_size])\n",
    "        y.append(seq[i + 1:i + block_size + 1])\n",
    "\n",
    "    return torch.tensor(x, dtype=torch.long).to(device), torch.tensor(y, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6oKqpa0zXkN"
   },
   "source": [
    "#Positional + Token Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9efiyQCzZ2k"
   },
   "source": [
    "Here we make 2 vectors, one having the token embedings of characters in the block while other has positions order\n",
    "\n",
    "---\n",
    "\n",
    "vocab_size is vocublary size (numbers of characters in this case)\n",
    "block_size is the max number of character to check at a time.\n",
    "embed_dim is the number of elements in a vector.\n",
    "\n",
    "---\n",
    "\n",
    "first we make a vector having the embedings for each token.\n",
    "[vocab_size x embed_dim]\n",
    "=> [8x4] = 2D vector having 32 elements each row is a token.\n",
    "\n",
    "\n",
    "each element is a uniform or normal distribution random values like:\n",
    "[\n",
    " [ 0.02, -0.11,  0.05, 0.07],  # token ID 0\n",
    " [ 0.12, -0.55,  0.33, 0.98],  # token ID 1\n",
    " [ 0.66,  0.20, -0.50, 0.44],  # token ID 2\n",
    " ...\n",
    "]\n",
    "\n",
    "\n",
    "These are updated during training using:\n",
    "new_value = old_value - learning_rate * gradient\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "position_embed is also a vector which just gives the position order\n",
    "\n",
    "\n",
    "For Example: if block size is 4 then each vector will be of 4 elements and position emebed vector will always be:  [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1754631000035,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "p2I0rVNKJz9b"
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "  def __init__(self, voacb_size, embed_dim, block_size ):\n",
    "    super().__init__()\n",
    "    self.token_embed=nn.Embedding(voacb_size,embed_dim)\n",
    "    self.position_embed=nn.Embedding(block_size,embed_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    B,T=x.shape\n",
    "    tok=self.token_embed(x)\n",
    "    pos=self.position_embed(torch.arange(T, device=x.device))\n",
    "    return tok+pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agh1-lSNy1Lf"
   },
   "source": [
    "#Self Attention + Casual Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MW7H_2dy6KZ"
   },
   "source": [
    "\n",
    "Query (q): “Which type of info i am finding?”\n",
    "\n",
    "Key (k): “Which type of info i hold?”\n",
    "\n",
    "Value (v): \"What actual info i hold?”\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Self attention is like you are in a group discussion and you listen to other and analyze who looks smarter of them and you focus more on his/her words and you talk according to the scenario and the last talk.\n",
    "\n",
    "wei= (q @ k.transpose(-2,-1)) * (C ** -0.5)\n",
    "\n",
    "This line compares current words with past words/other words to check how good they are according to the scenario and dividing by Sq.root(C) is just a math normalazier.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "We mask the future words like we don't know wtf other person will say after that so we just focus on previous words.\n",
    "\n",
    "After that we just normalize our answer (model output) using softmax and\n",
    "out = wei @ v\n",
    "\n",
    "is weighted sum. model actual output is build by analyzing which token depends mostly on which token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1754631002407,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "2Oqf9H-GMdBn"
   },
   "outputs": [],
   "source": [
    "class selfAttention(nn.Module):\n",
    "  def __init__(self,embed_dim,head_size) -> None:\n",
    "    super().__init__()\n",
    "    self.key=nn.Linear(embed_dim,head_size)\n",
    "    self.val=nn.Linear(embed_dim,head_size)\n",
    "    self.query=nn.Linear(embed_dim,head_size)\n",
    "    self.proj=nn.Linear(head_size,embed_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    B,T,C=x.shape\n",
    "    k=self.key(x)\n",
    "    v=self.val(x)\n",
    "    q=self.query(x)\n",
    "\n",
    "    wei= (q @ k.transpose(-2,-1)) * (C ** -0.5)\n",
    "\n",
    "    mask = torch.tril(torch.ones(T,T)).to(x.device) == 0\n",
    "    wei = wei.masked_fill(mask,float('-inf'))\n",
    "\n",
    "    wei = torch.softmax(wei, dim=-1)\n",
    "    out = wei @ v\n",
    "\n",
    "    return self.proj(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFny43CGyS2R"
   },
   "source": [
    "#Decoder Block + Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uABZLmQ4yeiv"
   },
   "source": [
    "Here we first get the self attention (mix token with contexual tokens) then we make the vector 4x and use ReLU activation to make negative values zero and add non-linearity so model can learn better. After that vector is again converted to original size having richer information.\n",
    "\n",
    "nl1 and nl2 are normalization layers, they normalize output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1754631004816,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "FT6YGtRauQHW"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  def __init__(self,embed_dim,head_size):\n",
    "    super().__init__()\n",
    "    self.sa=selfAttention(embed_dim,head_size)\n",
    "    self.ff=nn.Sequential(\n",
    "        nn.Linear(embed_dim,4*embed_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4*embed_dim,embed_dim)\n",
    "    )\n",
    "\n",
    "    self.nl1=nn.LayerNorm(embed_dim)\n",
    "    self.nl2=nn.LayerNorm(embed_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = x + self.sa(self.nl1(x))\n",
    "    x = x + self.ff(self.nl2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvPYkXpP0JoB"
   },
   "source": [
    "#Final GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANIqvy85mhY1"
   },
   "source": [
    "It is the execution point where it first do **embeding** then train it using **Block** function and then **normalize** it after that a linear layer is used to convert the whole calculation into the actual word to be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1754631006597,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "NlFJabzYmnJg"
   },
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "  def __init__(self,vocab_size,embed_dim,block_size,n_heads,n_layers):\n",
    "    super().__init__()\n",
    "    self.embed=Embedding(vocab_size,embed_dim,block_size)\n",
    "    self.blocks=nn.Sequential(*[Block(embed_dim, embed_dim//n_heads) for _ in range(n_layers)])\n",
    "    self.ln_f=nn.LayerNorm(embed_dim)\n",
    "    self.head=nn.Linear(embed_dim,vocab_size)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x=self.embed(x)\n",
    "    x=self.blocks(x)\n",
    "    x=self.ln_f(x)\n",
    "    return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQzvZghfK_Ak"
   },
   "source": [
    "#Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1754631010332,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "5C3jU7vMLCmW"
   },
   "outputs": [],
   "source": [
    "# Generation function\n",
    "def generate(model, start_str, max_length=20, temperature=1.0):\n",
    "    model.eval()\n",
    "    tokens = encode(start_str)\n",
    "    for _ in range(max_length):\n",
    "        # Get last block_size tokens\n",
    "        input_tokens = tokens[-block_size:]\n",
    "        input_tensor = torch.tensor([input_tokens]).to(model.head.weight.device) # Move to the same device as the model\n",
    "\n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)\n",
    "\n",
    "        # Get probabilities\n",
    "        probs = torch.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        tokens.append(next_token)\n",
    "\n",
    "    return decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xJ2avBuxE7K"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 441591,
     "status": "ok",
     "timestamp": 1754631474223,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "s4eVGvGux03J",
    "outputId": "dc08e6ec-0a27-4bd8-e79b-dd557b651cb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4529\n",
      "Epoch 10, Loss: 0.2059\n",
      "Epoch 20, Loss: 0.1322\n",
      "Epoch 30, Loss: 0.1900\n",
      "Epoch 40, Loss: 0.1669\n",
      "Epoch 50, Loss: 0.1784\n",
      "Epoch 60, Loss: 0.1642\n",
      "Epoch 70, Loss: 0.1844\n",
      "Epoch 80, Loss: 0.2232\n",
      "Epoch 90, Loss: 0.1607\n",
      "was a peaceful evening,\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(chars)\n",
    "embed_dim = 64\n",
    "block_size = 16\n",
    "n_heads = 4\n",
    "n_layers = 4\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "epochs = 100\n",
    "\n",
    "# Create model\n",
    "# Model setup\n",
    "model = MiniGPT(vocab_size, embed_dim, block_size, n_heads, n_layers).to(device)\n",
    "\n",
    "# Create dataset\n",
    "text = \"\"\"The sun was setting behind the hills. Birds flew across the orange sky, heading home. A boy sat on the grass, watching the clouds move slowly. He smiled, feeling the cool wind on his face. It was a peaceful evening, and everything felt calm. He didn’t want the moment to end.\n",
    "\n",
    " \"\"\" * 100  # Small repeating dataset for testing\n",
    "X, Y = get_batch(encode(text), block_size)\n",
    "X, Y = X.to(device), Y.to(device)\n",
    "dataset = TensorDataset(X, Y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # Move to GPU\n",
    "\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), batch_y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Generation test\n",
    "print(generate(model, \"was\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UCOqun6yFFF"
   },
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1754631586191,
     "user": {
      "displayName": "Akhyar",
      "userId": "06245746227071034024"
     },
     "user_tz": -300
    },
    "id": "ksmb3tYVyGl0",
    "outputId": "a615f14c-8945-4c4e-e251-e7b129b4727a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test generation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate\u001b[49m(model, \u001b[33m\"\u001b[39m\u001b[33mwas\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'generate' is not defined"
     ]
    }
   ],
   "source": [
    "# Test generation\n",
    "print(generate(model, \"was\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMBqK9T3Ynz2PwmFVaQLIrY",
   "collapsed_sections": [
    "UjimNV-d0SNi",
    "1ohSPIA4z0r1",
    "yatVHilO6nH5",
    "Ot1KZgzPzxon",
    "LeEw0cXwzq8x",
    "X6oKqpa0zXkN",
    "agh1-lSNy1Lf",
    "JFny43CGyS2R",
    "CvPYkXpP0JoB",
    "sQzvZghfK_Ak",
    "_UCOqun6yFFF"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1GIQ8CB1Wj0Wx0kT1L_MHhv52RwiTdyN1",
     "timestamp": 1754546492485
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
